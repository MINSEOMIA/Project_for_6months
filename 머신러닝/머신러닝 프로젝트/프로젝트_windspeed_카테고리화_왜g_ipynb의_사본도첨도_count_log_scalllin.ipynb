# -*- coding: utf-8 -*-
"""프로젝트 #windspeed #카테고리화 #왜도첨도 count log_scallling  .ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E5mz_0KMyMVa95C7GaU_kVxHKA2mOczZ
"""

#1단계 import package
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import os
import seaborn as sns
import pandas as pd
import datetime as dt
import scipy
import missingno as msno 
plt.style.use('seaborn')


train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')
submission = pd.read_csv('/content/sampleSubmission.csv')

#classifiaction.
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC,SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

#regression
from sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV
from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor

#model selection
from sklearn.model_selection import train_test_split,cross_validate
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV

#evaluation metrics
from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification

"""데이터 탐색
- datetime - hourly date + timestamp  

- season -  1 = spring, 2 = summer, 3 = fall, 4 = winter 
- holiday - whether the day is considered a holiday
-workingday - whether the day is neither a weekend nor holiday
- weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy
2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds >> 카테고리형
4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 
- temp - temperature in Celsius
- atemp - "feels like" temperature in Celsius
- humidity - relative humidity
- windspeed - wind speed
- casual - number of non-registered user rentals initiated
- registered - number of registered user rentals initiated
- count - number of total rentals << target

#데이터 탐색

## Datetime 년 월 일 시간 요일로 해체하기
"""

#datetime 컬럼 문자열 datetime으로 변경해야함...
train['datetime'] = pd.to_datetime(train['datetime'])
test['datetime'] = pd.to_datetime(test['datetime'])

train.info()

#년,월,일,시간,요일 추출
train['year']=train['datetime'].dt.year # 년도 추출 
train['month']=train['datetime'].dt.month # 월도 추출 
train['day']=train['datetime'].dt.day # 일 추출 
train['hour']=train['datetime'].dt.hour # 시 추출 
train['dayofweek']=train['datetime'].dt.dayofweek #요일 추출 , 월요일:0, 일요일:6

#불필요한 컬럼 드랍
#datatime object+ 위에서 따로 추출해내서 필요없음 
train.drop(["datetime"], axis = 1, inplace = True)

test['year']=test['datetime'].dt.year # 년도 추출 
test['month']=test['datetime'].dt.month # 월도 추출 
test['day']=test['datetime'].dt.day # 일 추출 
test['hour']=test['datetime'].dt.hour # 시 추출 
test['dayofweek']=test['datetime'].dt.dayofweek #요일 추출 , 월요일:0, 일요일:6

#datatime object+ 위에서 따로 추출해내서 필요없음 
test.drop(["datetime"], axis = 1, inplace = True)

import matplotlib.pyplot as plt
import seaborn as sns

"""##데이터 전처리

2012년에 수요량 증가.
-> 단 두개의 year만 비교가능하기때문에 수요가 앞으로도 지속증가하는지에 대한 여부는 알 수 없음,

비교적 12, 1, 2월에 자전거 수요량이 적음 -> 추운기온, 겨울
6, 7, 8,9월에 자전거 수요량이 가장 많음 -> 따듯한기온,여름

day별로 명백한 차이가 없음

월별로 출력했을 때 분명, 12,1,2월(겨울)에 가장 수요량이 적었는데, 

위 season 그래프와는 다르게 나왔다. 

따라서 season을 나누는 경계가 다름을 알 수 있었다.
"""

#season이 월별로 123 / 456 /789 /101112 로 나눠짐...
print(train[train['season'] == 1].month.unique())
print(train[train['season'] == 2].month.unique())
print(train[train['season'] == 3].month.unique())
print(train[train['season'] == 4].month.unique())

# 날씨와 count의 상관관계 확인 전, season을 올바른 데이터로 바꿔주기
# 워싱턴 D.C의 평균 기온 자료에 기반하여 세계절씩 봄/여름/가을/겨울로 나눈다.

# 계절 분류
def change_season(month):
  if month in [12,1,2]:
    return 4
  elif month in [3,4,5]:
    return 1
  elif month in [6,7,8]:
    return 2
  else:
    return 3
train['season'] = train.month.apply(change_season)

# 날씨와 count의 상관관계 확인 전, season을 올바른 데이터로 바꿔주기
# 워싱턴 D.C의 평균 기온 자료에 기반하여 세계절씩 봄/여름/가을/겨울로 나눈다.

# 계절 분류
def change_season(month):
  if month in [12,1,2]:
    return 4
  elif month in [3,4,5]:
    return 1
  elif month in [6,7,8]:
    return 2
  else:
    return 3
test['season'] = test.month.apply(change_season)

# 날씨와 count의 상관관계 확인 전, season을 올바른 데이터로 바꿔주기
# 워싱턴 D.C의 평균 기온 자료에 기반하여 세계절씩 봄/여름/가을/겨울로 나눈다.

# 계절 분류
def change_season(month):
  if month in [12,1,2]:
    return 4
  elif month in [3,4,5]:
    return 1
  elif month in [6,7,8]:
    return 2
  else:
    return 3
test['season'] = test.month.apply(change_season)

"""시간대별로 보았을때, 
8시 , 17시경에 가장 자전거 수요량이 많다. (출, 퇴근시간임을 알 수 있다.)

1: 근무일 / 0: 근무일 아닐 때

근무일에는 출퇴근 시간에 수요량이 급증하고, 
근무일이 아닐 때에는 오후 낮 시간대에 수요량이 증가

따라서 holiday 변수도 예측에 영향을 줄 것이나 workingday 그래프와 비슷한 양상을 보일것이라고 예측

아주깨끗한날씨(1) 약간의 안개와 구름(2) 약간의 눈,비(3) 아주많은비와 우박(4)
1,2번날씨 > 3번날씨 > 4번날씨는 데이터가 거의 없음

dayofweek 의 모든 양상은 비슷하게 나타남....
"""

#바람세기가 0인 값 측정
len(train[train['windspeed']==0])
# 결과 : 1313

"""바람세기 0 인걸 대체해야함
#Feature engineering
이상치 제거

#iQR 방식을 사용한 이상치 제거
전체 데이터를 오름차순으로 정리한 후, 정확히 4등분한다. (25%, 50%, 75%, 100%) 

75%지점값 - 25%지점값의 = iQR

그외 이상과 이하에 해당하는 값 = 이상치

....................40정도 지워짐.......

# 왜도 첨도 확인

왜도0 첨도3
데이터의 분포가 한쪽으로 쏠린것을 의미
skew의 수치가 -2~+2 정도가 되어야 치우침이 없는 데이터
-2이하는 왼쪽으로 쏠린 데이터 (negative) +2 이상은 오른쪽으로 쏠린 데이터(positive)
positive일경우 변환방법 : square root, cube, log(밑10)
negative일경우 변환방법 : square, cube root, logarithmic(밑2인log)
첨도
분포의 뾰족함이나 평평함에 관련된 것이 아니라 분포의 tail에 대한 모든 것
한쪽 꼬리부분의 극값과 다른쪽 꼬리의 극값과의 차이를 보여줌
아웃라이어를 찾을 때 주로 사용
첨도가 높다 -> 아웃라이어가 많이 있다
"""

#count log_scalling 으로 정규화
#lambda 를 사용해서 로그를 취해준 count값을 count_log 컬럼으로 생성
train['count_log'] = train['count'].map(lambda i:np.log(i) if i > 0 else 0)

fig, ax = plt.subplots(1,1, figsize = (10, 6))
graph = sns.distplot(train['count_log']
			, color = 'b'
            , label = 'skewness: {:2f}'.format(train['count_log'].skew())
            , ax = ax)
graph = graph.legend(loc = 'best')

print("skewness(왜도): %f" %train['count_log'].skew())
print("kurtosis(첨도): %f" %train['count_log'].kurt())

#필요없는 count값 없애주기
train.drop('count', axis = 1, inplace = True)

"""count가 0에 치우쳐짐

변수들의 크기 차이에 민감한 모델을 사용한다면 Scaling 
타겟변수 count 에만 Log를 취함
왜도와 첨도의 수치도 정규화
데이터가 0 하나에만 치우쳐져 있는 것이 개선된 것을 확인

앞으로는 count_log 값을 예측에 활용해야함

#windspeed 0값 개선
풍속은 상식적으로 아예 0으로 관측되기 어려움
조금만 바람이 불어도 0,01로는 나와야하는거 아닌가
아님 관측이 안돼서 결측치로 0을 넣은 걸수도

<< 결측값 처리 방법>>

- 결측값을 앞 방향 혹은 뒷 방향으로 채우기
- 결측값을 변수별 평균으로 채우기
- 결측값을 가진 데이터를 삭제하기
- 결측값을 0 이나, 아예 다른 값으로 (-999) 대체하기
- 결측값을 예측된 값으로 대체하기 (머신러닝을 돌려서)


(1)선택한 첫번째 방법
 0이 아닌값과 날씨변수로 예측해서 값 채워넣기

windspeed가 null값인 경우를 0으로 처리한 것이라면
결측치를 채우는 방법중 예측된 값으로 대체하는 방법으로 0을 대체
"""

from sklearn.ensemble import RandomForestClassifier
# RandomForest로 값을 예측해
def predict_windspeed(data):
    dataWind0 = data.loc[data["windspeed"] == 0]
    dataWindNot0 = data.loc[data["windspeed"] != 0]
    # 풍속 데이터를 0인 값과 아닌 값들로 나눠준다

    wcol = ["season", "weather", "humidity", "day", "temp", "atemp"]
    # 예측에 필요한 변수들을 지정합니다. 
    # 풍속이 날씨에 대한 변수이기 때문에  날씨에 관련된 변수들을 예측의 독립변수로 지정

    dataWindNot0["windspeed"] = dataWindNot0["windspeed"].astype("str")
    # 풍속이 0이 아닌 값들을 string데이터로 변환

    rf_wind = RandomForestClassifier()
    rf_wind.fit(dataWindNot0[wcol], dataWindNot0["windspeed"])
    wind0 = rf_wind.predict(X=dataWind0[wcol])
    # 랜덤포레스트를 사용하여 wcol의 변수들로 0인 값들을 학습, 예측
    
    predictWind0 = dataWind0
    predictWindNot0 = dataWindNot0
    # 예측한 값을 넣어줄 데이터프레임을 만듦

    predictWind0["windspeed"] = wind0
    # 예측값들을 넣어줌

    data = predictWindNot0.append(predictWind0)
    # 0이 아닌 데이터들과 예측값들을 합쳐줌

    data["windspeed"] = data["windspeed"].astype("float")
    # 풍속의 데이터들을 다시 float 타입으로 변환
    
    data.reset_index(inplace = True)
    data.drop("index", inplace = True, axis = 1)
    
    return data

train = predict_windspeed(train)
test = predict_windspeed(test)

#독립변수 + 0이 아닌 데이터로 분류했던 windspeed를 RandomForest로 학습
#학습시킨 모델을 통해 0 값을 가졌던 windspeed를 예측

fig, (ax1, ax2) = plt.subplots(2,1)
fig.set_size_inches(20,15)

#갯수를 세야하니 countplot
sns.countplot(data = train, x = 'windspeed', ax = ax1)
sns.countplot(data = test, x = 'windspeed', ax = ax2)

"""##결론

Bike Sharing Demand EDA에서는 뚜렷한 특징을 보이는 Feature

 -시간(hour): 출퇴근 시간인 8시와 17~18시에 대여량이 가장 많다

-요일(dayofweek): 주중에 출퇴근, 주말에는 오후 시간대에 대여량이 많다

-계절(season): 여름. 

-년도(year): 2011<2012



drop:

atemperature (0.98)

holiday, dayofweek (workingday만 가져가기)

#One hot encoding

#Modeling 전 변수 정리 (drop 실행)
"""

# 연속형 feature와 범주형 feature
# 범주형 feature의 type을 category로 변경 해 준다.
categorical_feature_names = ['season',"holiday","workingday","weather",
                             "dayofweek","month","year","hour"]
for var in categorical_feature_names:
    train[var] = train[var].astype("category")
    test[var] = test[var].astype("category")
feature_names = [ 'season',"weather", "temp", "atemp", "humidity",
                 "year", "hour", "dayofweek", "holiday", "workingday"
                ]

feature_names

categorical_feature_names = ['season',"holiday","workingday","weather",
                             "dayofweek","month","year","hour"]

for var in categorical_feature_names:
    train[var] = train[var].astype("category")
    test[var] = test[var].astype("category")

X_train = train[feature_names]

print(X_train.shape)
X_train.head()

X_test = test[feature_names]

print(X_test.shape)
X_test.head()

label_name = "count_log"

y_train = train[label_name]

print(y_train.shape)
y_train.head()

#train = pd.get_dummies(train, columns=['year', 'month', 'day', 'hour', 'holiday', 'workingday'])
#test = pd.get_dummies(test, columns=['year', 'month', 'day', 'hour', 'holiday', 'workingday'])

#train에 사용될 변수

#submission의 형태를 살펴보았을 때, datetime을 기준으로 예측값을 적엇다.
#따라서 test의 datetime은 미래의 submission 을 위해서 따로 저장해두기로 한다.
test_datetime = test['datetime']

train.drop(['atemp', 'holiday','dayofweek'], axis = 1, inplace = True)
test.drop(['atemp', 'holiday','dayofweek'], axis = 1, inplace = True)

train.columns

train.info()
#-범주형 데이터 int64 category로 변경해야 하나?
#feature_names=['season','holiday','workingday','weather','temp','atemp',
              #'humidity','windspeed','year','hour','dayofweek']

#-수리형을 범주형으로 바꿔주는 for 문 
#c_f_n =['season','holiday','workingday','weather',
        #'year','hour','dayofweek','month']

#for v in c_f_n:
    #-dtype이 int64라서 category로 변경
    #train[v]=train[v].astype("category")
    #test[v]=train[v].astype("category")
#train.info() #- dtype 변경 확인 

#- 4개의 변수로 되어있음 확인 
#train['season'].dtypes 
#test['season'].dtypes

X = train.drop('count_log', axis = 1)
y = train['count_log']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""##모델링 -시도(2) rmsle 만들어서 돌리기

"""

lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
pred = lr_reg.predict(X_test)

y_test_exp = np.expm1(y_test)
pred_exp = np.expm1(pred)
print('LinearRegression RMSLE:', rmsle(y_test_exp, pred_exp))

from sklearn.ensemble import RandomForestRegressor
randomforest_model = RandomForestRegressor(n_estimators=100)

y_train_log = np.log1p(y_train)
randomforest_model.fit(X_train, y_train_log)

preds = randomforest_model.predict(X_train)
score_rf = rmsle(np.exp(y_train_log),np.exp(preds))
print ("RMSLE Value For Random Forest: ", score_rf)
predsTest = randomforest_model.predict(X_test)



from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
rf_reg = RandomForestRegressor(n_estimators=500)
rf_reg.fit(X_train, y_train_log)
pred = rf_reg.predict(X_test)

xgb_reg = XGBRegressor(n_estimators=500)
xgb_reg.fit(X_train, y_train_log)
pred = xgb_reg.predict(X_test)

y_test_exp = np.expm1(y_test)
pred_exp = np.expm1(pred)
print('XGBRegressor RMSLE:', rmsle(y_test_exp, pred_exp))

y_test_exp = np.expm1(y_test)
pred_exp = np.expm1(pred)
print('RandomForestRegressor RMSLE:', rmsle(y_test_exp, pred_exp))

#6 주영님
#https://github.com/BaekKyunShin/Kaggle/blob/master/Bike_Sharing_Demand/Bike%20Sharing%20Demand%20by%20Random%20Forest.ipynb
from sklearn.metrics import make_scorer
def RMSLE(predicted_values, actual_values):
    predicted_values = np.array(predicted_values)
    actual_values = np.array(actual_values)
    
    log_predict = np.log(predicted_values + 1)
    log_actual = np.log(actual_values + 1)
    
    difference = log_predict - log_actual
    difference = np.square(difference)
    
    mean_difference = difference.mean()
    
    score = np.sqrt(mean_difference)
    
    return score
    
RMSLE_scorer = make_scorer(RMSLE)
RMSLE_scorer

#선형회귀 모델
 from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
import warnings
pd.options.mode.chained_assignment = None
warnings.filterwarnings('ignore', category=DeprecationWarning)

# 선형회귀 모델을 초기화
linear_model = LinearRegression()

# 모델 학습
y_train_log = np.log1p(y_train)
linear_model.fit(X_train, y_train_log)

# 예측하고 정확도를 평가한다.
preds = linear_model.predict(X_train)
print("RMSLE Value for linear regression: ",
     RMSLE(np.exp(y_train_log), np.exp(preds)))

#릿지 모델
ridge_model = Ridge()
ridge_params_ = {'max_iter':[3000],
                 'alpha':[0.01, 0.1, 1, 2, 3, 4, 10,
                          30,100,200,300,400,800,900,1000]}
rmsle_scorer = metrics.make_scorer(RMSLE, greater_is_better=False)
grid_ridge_model = GridSearchCV(ridge_model,
                          ridge_params_,
                          scoring = rmsle_scorer,
                          cv=5)

y_train_log = np.log1p(y_train)
grid_ridge_model.fit(X_train, y_train)
preds = grid_ridge_model.predict(X_train)
print (grid_ridge_model.best_params_)
print ("RMSLE Value For Ridge Regression: ", 
       RMSLE(np.exp(y_train),np.exp(preds)))

lasso_model = Lasso()

alpha  = 1/np.array([0.1, 1, 2, 3, 4, 10, 30,100,200,300,400,800,900,1000])
lasso_params_ = {'max_iter':[3000], 'alpha':alpha}

grid_lasso_model = GridSearchCV(lasso_model,
                          lasso_params_,
                          scoring = rmsle_scorer,
                          cv=5)

y_train_log = np.log1p(y_train)
grid_lasso_model.fit(X_train, y_train_log)
preds = grid_lasso_model.predict(X_train)
print (grid_lasso_model.best_params_)#windspeed #카테고리화 #왜도첨도 count log_scallling
print ("RMSLE Value For Lasso Regression: ", 
       RMSLE(np.exp(y_train_log),np.exp(preds)))

from sklearn.ensemble import RandomForestRegressor
randomforest_model = RandomForestRegressor(n_estimators=100)

y_train_log = np.log1p(y_train)
randomforest_model.fit(X_train, y_train_log)

preds = randomforest_model.predict(X_train)
score_rf = RMSLE(np.exp(y_train_log),np.exp(preds))
print ("RMSLE Value For Random Forest: ", score_rf)
predsTest = randomforest_model.predict(X_test)

fig,(ax1,ax2)= plt.subplots(ncols=2)
fig.set_size_inches(12,5)
sns.distplot(y_train,ax=ax1,bins=50)
sns.distplot(np.exp(predsTest),ax=ax2,bins=50)

from sklearn.ensemble import GradientBoostingRegressor
gbm = GradientBoostingRegressor(n_estimators=4000, alpha=0.01);

y_train_log = np.log1p(y_train)
gbm.fit(X_train, y_train_log)

preds = gbm.predict(X_train)
score = RMSLE(np.exp(y_train_log),np.exp(preds))
print ("RMSLE Value For Gradient Boost: ", score)